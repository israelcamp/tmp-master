{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT2STRIDE = {\n",
    "    16: [2] * 4 + [1],\n",
    "    32: [2] * 5,\n",
    "    64: [4] + [2] * 4,\n",
    "    128: [4] * 2 + [2] * 3,\n",
    "    256: [4] * 3 + [2] * 2,\n",
    "    512: [4] * 4 + [2] * 1, \n",
    "}\n",
    "\n",
    "for k, v in HEIGHT2STRIDE.items():\n",
    "    assert reduce(lambda x, y: x*y, v) == k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = open('vocab.txt').read().splitlines()\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_list = HEIGHT2STRIDE[SIZE]\n",
    "images_path = f'images_size/size={SIZE}/'\n",
    "with open(\"size2labels.json\", 'rb') as f:\n",
    "    size2labels = json.load(f)\n",
    "labels = size2labels[str(SIZE)]\n",
    "val_labels = {k:v for k, v in labels.items() if int(k) < 100}\n",
    "train_labels = {k:v for k, v in labels.items() if int(k) >= 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 899)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision as tv\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, images_path, labels):\n",
    "        self.images_path = images_path\n",
    "        self.labels = labels\n",
    "        self.keys = sorted(list(labels.keys()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.keys[idx]\n",
    "        image_path = os.path.join(self.images_path, f'{idx}.png')\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        image = tv.transforms.ToTensor()(image)\n",
    "        \n",
    "        label = self.labels[str(idx)]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SimpleDataset(images_path, train_labels)\n",
    "val_dataset = SimpleDataset(images_path, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from model import CNN\n",
    "from ctc import GreedyCTCDecoder\n",
    "from metrics import compute_f1, compute_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GreedyCTCDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch):\n",
    "    images, texts = batch\n",
    "    images = images.to(device)\n",
    "\n",
    "    y = [[\n",
    "        vocab.index(t) for t in txt\n",
    "    ] for txt in texts]\n",
    "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "    return images, texts, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctc_loss(logits, y):\n",
    "    logits = logits.permute(1, 0, 2).log_softmax(2)\n",
    "    input_lengths = torch.full(\n",
    "        size=(logits.size(1),),\n",
    "        fill_value=logits.size(0),\n",
    "        dtype=torch.int32,\n",
    "    )\n",
    "    target_lengths = torch.full(\n",
    "        size=(y.size(0),),\n",
    "        fill_value=y.size(1),\n",
    "        dtype=torch.int32,\n",
    "    )\n",
    "    criterion = torch.nn.CTCLoss(zero_infinity=True)\n",
    "    loss = criterion(logits, y, input_lengths, target_lengths)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(logits):\n",
    "    yp = logits.argmax(-1)\n",
    "    pt = []\n",
    "    for row in yp:\n",
    "        predictions = decoder(row, None)\n",
    "        pt.append(''.join(vocab[p] for p in predictions))\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(stride_list, vocab_size)\n",
    "_ = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 7.624668448334674, val loss: 6.784406604766846, val f1: 0.0, val em: 0.0\n",
      "epoch: 1, train loss: 6.463653319934849, val loss: 6.827717645168304, val f1: 0.0, val em: 0.0\n",
      "epoch: 2, train loss: 5.818309278898342, val loss: 5.03540125131607, val f1: 0.0, val em: 0.0\n",
      "epoch: 3, train loss: 5.41338135847393, val loss: 5.21288158416748, val f1: 0.0, val em: 0.0\n",
      "epoch: 4, train loss: 5.1282942871628405, val loss: 4.893181304931641, val f1: 0.0, val em: 0.0\n",
      "epoch: 5, train loss: 4.912418022061172, val loss: 4.50979599237442, val f1: 0.0, val em: 0.0\n",
      "epoch: 6, train loss: 4.736394246962308, val loss: 4.206970593929291, val f1: 0.0, val em: 0.0\n",
      "epoch: 7, train loss: 4.584238116879087, val loss: 4.381089975833893, val f1: 0.003333333333333333, val em: 0.0\n",
      "epoch: 8, train loss: 4.442851871307311, val loss: 4.0338801407814024, val f1: 0.0022222222222222222, val em: 0.0\n",
      "epoch: 9, train loss: 4.301988635551147, val loss: 3.904453282356262, val f1: 0.012523809523809524, val em: 0.0\n",
      "epoch: 10, train loss: 4.156683585137229, val loss: 3.878299416303635, val f1: 0.014603174603174604, val em: 0.0\n",
      "epoch: 11, train loss: 4.011558238426579, val loss: 3.8941727459430693, val f1: 0.016936507936507937, val em: 0.0\n",
      "epoch: 12, train loss: 3.860088757779811, val loss: 3.6956211972236632, val f1: 0.02846031746031746, val em: 0.0\n",
      "epoch: 13, train loss: 3.7045086978298483, val loss: 3.6868641102313995, val f1: 0.022714285714285715, val em: 0.0\n",
      "epoch: 14, train loss: 3.5470385325609155, val loss: 3.680128961801529, val f1: 0.02604761904761905, val em: 0.0\n",
      "epoch: 15, train loss: 3.390226768747229, val loss: 3.686445531845093, val f1: 0.03413095238095238, val em: 0.0\n",
      "epoch: 16, train loss: 3.237049068215005, val loss: 3.9589863228797912, val f1: 0.035515873015873015, val em: 0.0\n",
      "epoch: 17, train loss: 3.088912556611601, val loss: 3.6079738795757295, val f1: 0.028509803921568624, val em: 0.0\n",
      "epoch: 18, train loss: 2.948875141956161, val loss: 3.7209228479862215, val f1: 0.043893901420217214, val em: 0.0\n",
      "epoch: 19, train loss: 2.817106525230328, val loss: 3.6849741369485853, val f1: 0.05007142857142857, val em: 0.0\n"
     ]
    }
   ],
   "source": [
    "gradient_steps = 32\n",
    "train_losses = []\n",
    "val_epoch = {\n",
    "    \"loss\": [],\n",
    "    \"f1\": [],\n",
    "    \"em\": [],\n",
    "}\n",
    "for epoch in range(100):\n",
    "\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        images, texts, y = prepare_batch(batch)\n",
    "        logits = model(images)\n",
    "        loss = get_ctc_loss(logits, y)\n",
    "        loss.backward()\n",
    "        if idx % gradient_steps == 0 or idx == len(train_loader) - 1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        val_f1 = []\n",
    "        val_em = []\n",
    "        for batch in val_loader:\n",
    "            images, texts, y = prepare_batch(batch)\n",
    "            logits = model(images)\n",
    "            pt = get_predictions(logits)\n",
    "            loss = get_ctc_loss(logits, y)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            for t, p in zip(texts, pt):\n",
    "                val_f1.append(compute_f1(t, p))\n",
    "                val_em.append(compute_exact(t, p))\n",
    "    val_epoch[\"loss\"].append(sum(val_losses) / len(val_losses))\n",
    "    val_epoch[\"f1\"].append(sum(val_f1) / len(val_f1))\n",
    "    val_epoch[\"em\"].append(sum(val_em) / len(val_em))\n",
    "    print(f'epoch: {epoch}, train loss: {sum(train_losses) / len(train_losses)}, val loss: {sum(val_losses) / len(val_losses)}, val f1: {sum(val_f1) / len(val_f1)}, val em: {sum(val_em) / len(val_em)}')\n",
    "\n",
    "torch.save(model.state_dict(), f'cnn_size={SIZE}.pth')\n",
    "history = val_epoch\n",
    "history.update({\n",
    "    \"train_losses\": train_losses\n",
    "})\n",
    "with open(f'cnn_size={SIZE}.json', 'w') as f:\n",
    "    json.dump(val_epoch, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
