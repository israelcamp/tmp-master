{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT2STRIDE = {\n",
    "    16: [2] * 4 + [1],\n",
    "    32: [2] * 5,\n",
    "    64: [4] + [2] * 4,\n",
    "    128: [4] * 2 + [2] * 3,\n",
    "    256: [4] * 3 + [2] * 2,\n",
    "    512: [4] * 4 + [2] * 1, \n",
    "}\n",
    "\n",
    "for k, v in HEIGHT2STRIDE.items():\n",
    "    assert reduce(lambda x, y: x*y, v) == k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = open('vocab.txt').read().splitlines()\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_list = HEIGHT2STRIDE[SIZE]\n",
    "images_path = f'images_size/size={SIZE}/'\n",
    "with open(\"size2labels.json\", 'rb') as f:\n",
    "    size2labels = json.load(f)\n",
    "labels = size2labels[str(SIZE)]\n",
    "val_labels = {k:v for k, v in labels.items() if int(k) < 100}\n",
    "train_labels = {k:v for k, v in labels.items() if int(k) >= 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 900)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels), len(train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision as tv\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, images_path, labels):\n",
    "        self.images_path = images_path\n",
    "        self.labels = labels\n",
    "        self.keys = sorted(list(labels.keys()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.keys[idx]\n",
    "        image_path = os.path.join(self.images_path, f'{idx}.png')\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        image = tv.transforms.ToTensor()(image)\n",
    "        \n",
    "        label = self.labels[str(idx)]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SimpleDataset(images_path, train_labels)\n",
    "val_dataset = SimpleDataset(images_path, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADkAAAAQCAIAAAAj2UAsAAAC00lEQVR4nO2Vu0srQRTGZ2Z3WdgYhCQGLYQUKqiNhYqdioWwgWCzQRFsrAQbLSx1W/8DrQQLUQwIauELEZFEQ7YQ7LKgErNm80AC7is7O3OLQJAU3nsLuXjxK4aZgXP4zTnfYSClFHwToX8N8Bf6Yf0a/b+snwwiIeTj5s9HlhDSiP0Y1bhsqJnVdV1N06rVKgAAY/zy8oIxBgB4nuc4DoSwVqthjC3L8jyPEKLreqVSKZVKCKFKpVKtVhFClmVBCF3XrSe0bbtQKJTLZYxxPVtdxWLx7e0NIYQQ0nXdNE0IoeM4hULBtm2EmtnYpvPGxoZt26Iotra2qqp6fX1dLpdXVlb29/dHRkY0Tbu4uJicnDw+Pp6dne3p6UkkEul0Oh6P8zyfTCYBAKIoZjIZSZJkWV5aWlIUJZ/Pv7+/T0xMaJoWCoVGR0cBAKqqbm5u+v3+qampp6enh4cH27bn5+dTqZSqqizLxmKxvr6+z1gblQMAdHd353K5UqlEKX19fQ0EAkdHR8FgsL+/PxgMtre3MwwzNzdnmmY0Gl1eXl5fX9d1fXd3NxKJXF1dUUozmYxhGG1tbRjjUCj0+PhYq9UIIQgh0zS7uro6OjqSyWSxWFxdXb27uzs7O/P5fJIkQQh3dnZkWf7MA4uLizMzM9vb2wAAhmHC4TDLsicnJ729vefn56ZpPj8/15/h9/sBAA0zsCwLIWQYhhAyMDBwcHAwPT19e3vb0tISCAQ4jvP5fBBCQRDqzRUEQVGUy8tLURQdx6mXCSFUXwkhHMf9xgOHh4f39/dDQ0MAgHQ6rSgKx3HZbDYWi/E8LwjC6ekppdTzvHpShBClFCE0NjYmyzKlNBqNRiIRwzAGBwcTiUQ4HHZdN5fLZbNZjuP29vby+Xw8HjcMY3x8vLOzM5VKDQ8Pr62tWZa1sLBwc3OztbXF87wkSc09//ljv0Q/rF+j78T6C+hchZUeA7TeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=57x16>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "image = tv.transforms.ToPILImage()(x)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from convmaxpool import ConvOCRMaxPool\n",
    "from ctc import GreedyCTCDecoder\n",
    "from metrics import compute_f1, compute_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GreedyCTCDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch):\n",
    "    images, texts = batch\n",
    "    images = images.to(device)\n",
    "\n",
    "    y = [[\n",
    "        vocab.index(t) for t in txt\n",
    "    ] for txt in texts]\n",
    "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "    return images, texts, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctc_loss(logits, y):\n",
    "    logits = logits.permute(1, 0, 2).log_softmax(2)\n",
    "    input_lengths = torch.full(\n",
    "        size=(logits.size(1),),\n",
    "        fill_value=logits.size(0),\n",
    "        dtype=torch.int32,\n",
    "    )\n",
    "    target_lengths = torch.full(\n",
    "        size=(y.size(0),),\n",
    "        fill_value=y.size(1),\n",
    "        dtype=torch.int32,\n",
    "    )\n",
    "    criterion = torch.nn.CTCLoss(zero_infinity=True)\n",
    "    loss = criterion(logits, y, input_lengths, target_lengths)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(logits):\n",
    "    yp = logits.argmax(-1)\n",
    "    pt = []\n",
    "    for row in yp:\n",
    "        predictions = decoder(row, None)\n",
    "        pt.append(''.join(vocab[p] for p in predictions))\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvOCRMaxPool(imgH=SIZE, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26, 61])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x.unsqueeze(0).to(device))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 3.6840924293764177, val loss: 3.695952899456024, val f1: 0.0, val em: 0.0\n",
      "epoch: 1, train loss: 3.603958364721135, val loss: 5.76888534784317, val f1: 0.0, val em: 0.0\n",
      "epoch: 2, train loss: 3.54373823774272, val loss: 3.826244411468506, val f1: 0.0, val em: 0.0\n",
      "epoch: 3, train loss: 3.47221166018119, val loss: 3.4640356266498564, val f1: 0.0, val em: 0.0\n",
      "epoch: 4, train loss: 3.4005590772734866, val loss: 3.52440234541893, val f1: 0.019395604395604397, val em: 0.0\n",
      "epoch: 5, train loss: 3.329606669630702, val loss: 3.421689361333847, val f1: 0.019298663324979114, val em: 0.0\n",
      "epoch: 6, train loss: 3.255032488770502, val loss: 3.514238007068634, val f1: 0.0, val em: 0.0\n",
      "epoch: 7, train loss: 3.177322292155498, val loss: 3.3608844566345213, val f1: 0.014714285714285714, val em: 0.0\n",
      "epoch: 8, train loss: 3.0995132284131426, val loss: 3.853304090499878, val f1: 0.007857142857142858, val em: 0.0\n",
      "epoch: 9, train loss: 3.021137339201599, val loss: 2.813670263290405, val f1: 0.03820028011204482, val em: 0.01\n",
      "epoch: 10, train loss: 2.9416660073372722, val loss: 3.5328351640701294, val f1: 0.02619047619047619, val em: 0.01\n",
      "epoch: 11, train loss: 2.8615606217515612, val loss: 2.804834192991257, val f1: 0.03334628237259816, val em: 0.0\n",
      "epoch: 12, train loss: 2.7808710608953, val loss: 2.845489350259304, val f1: 0.0478780284043442, val em: 0.01\n",
      "epoch: 13, train loss: 2.699630093548714, val loss: 2.8935053396224975, val f1: 0.03648913951545531, val em: 0.0\n",
      "epoch: 14, train loss: 2.619409792014171, val loss: 2.5653966015577314, val f1: 0.04801002506265664, val em: 0.01\n",
      "epoch: 15, train loss: 2.5397206003320827, val loss: 2.6579881572723387, val f1: 0.04963492063492063, val em: 0.0\n",
      "epoch: 16, train loss: 2.4609309531363652, val loss: 2.5301700910925864, val f1: 0.0564624060150376, val em: 0.01\n",
      "epoch: 17, train loss: 2.385273031439177, val loss: 2.5409945131093266, val f1: 0.07681746031746033, val em: 0.04\n",
      "epoch: 18, train loss: 2.3101699753529705, val loss: 2.794171395897865, val f1: 0.053269841269841266, val em: 0.0\n",
      "epoch: 19, train loss: 2.2364708576683947, val loss: 2.5790447037667037, val f1: 0.05840183792815372, val em: 0.01\n",
      "epoch: 20, train loss: 2.1642747077668827, val loss: 2.541983986720443, val f1: 0.08512698412698412, val em: 0.03\n",
      "epoch: 21, train loss: 2.094237679515082, val loss: 2.4841334645822646, val f1: 0.08511183261183261, val em: 0.02\n",
      "epoch: 22, train loss: 2.0272207187091005, val loss: 2.57473685875535, val f1: 0.0944515455304929, val em: 0.03\n",
      "epoch: 23, train loss: 1.9612537018056193, val loss: 3.094123311340809, val f1: 0.10816081871345029, val em: 0.05\n",
      "epoch: 24, train loss: 1.8969597190910654, val loss: 2.7868400095030665, val f1: 0.08660306068200806, val em: 0.03\n",
      "epoch: 25, train loss: 1.8354542862376575, val loss: 2.888000954464078, val f1: 0.0936002506265664, val em: 0.03\n",
      "epoch: 26, train loss: 1.7789478785847972, val loss: 3.091836190149188, val f1: 0.07452380952380952, val em: 0.04\n",
      "epoch: 27, train loss: 1.7242723886385567, val loss: 2.930411077383906, val f1: 0.13079573934837094, val em: 0.06\n",
      "epoch: 28, train loss: 1.6702952413790446, val loss: 2.9656164726545104, val f1: 0.12275613275613274, val em: 0.06\n",
      "epoch: 29, train loss: 1.6181272969632974, val loss: 3.141316621112637, val f1: 0.10743065998329156, val em: 0.05\n",
      "epoch: 30, train loss: 1.5685842283405873, val loss: 2.872712123405654, val f1: 0.11935714285714286, val em: 0.07\n",
      "epoch: 31, train loss: 1.5226768127671568, val loss: 3.3124181432649493, val f1: 0.09193931799194957, val em: 0.03\n",
      "epoch: 32, train loss: 1.4958091112531222, val loss: 3.092983381524682, val f1: 0.08264786967418546, val em: 0.03\n",
      "epoch: 33, train loss: 1.457253472407133, val loss: 2.7519824746949597, val f1: 0.1095497076023392, val em: 0.04\n",
      "epoch: 34, train loss: 1.4179838116953267, val loss: 2.9014909985486885, val f1: 0.12098473456368193, val em: 0.05\n",
      "epoch: 35, train loss: 1.3795989201974113, val loss: 2.8613023211073596, val f1: 0.13060526315789475, val em: 0.06\n",
      "epoch: 36, train loss: 1.3429678708896435, val loss: 3.1146207320934627, val f1: 0.13873433583959902, val em: 0.07\n",
      "epoch: 37, train loss: 1.3080722400593985, val loss: 3.056718912156066, val f1: 0.1288483709273183, val em: 0.06\n",
      "epoch: 38, train loss: 1.2748578348243265, val loss: 3.1067831740924157, val f1: 0.11638011695906433, val em: 0.05\n",
      "epoch: 39, train loss: 1.2432378607752725, val loss: 3.3218260328052565, val f1: 0.14115789473684212, val em: 0.07\n",
      "epoch: 40, train loss: 1.213113506026354, val loss: 3.381547369812615, val f1: 0.14843274853801172, val em: 0.08\n",
      "epoch: 41, train loss: 1.1844044400895153, val loss: 3.343465958595043, val f1: 0.11748830409356727, val em: 0.05\n",
      "epoch: 42, train loss: 1.1570088401118361, val loss: 3.6263075024983844, val f1: 0.11210526315789475, val em: 0.04\n",
      "epoch: 43, train loss: 1.1308572417230782, val loss: 3.3337662981671747, val f1: 0.13226900584795323, val em: 0.07\n",
      "epoch: 44, train loss: 1.1059944342819796, val loss: 4.0261554880265615, val f1: 0.089203425229741, val em: 0.03\n",
      "epoch: 45, train loss: 1.1012923939288874, val loss: 3.4520842012763024, val f1: 0.055819548872180456, val em: 0.02\n",
      "epoch: 46, train loss: 1.0817474879936528, val loss: 3.0638258511573078, val f1: 0.09403884711779448, val em: 0.03\n",
      "epoch: 47, train loss: 1.0607801259610297, val loss: 3.2749479673989117, val f1: 0.12181662489557224, val em: 0.05\n",
      "epoch: 48, train loss: 1.0396855452777147, val loss: 3.1026814122800714, val f1: 0.11909440267335006, val em: 0.06\n",
      "epoch: 49, train loss: 1.0191496093620647, val loss: 3.129950190165546, val f1: 0.11928696741854637, val em: 0.05\n",
      "epoch: 50, train loss: 0.9993361815612007, val loss: 3.4630065653822384, val f1: 0.13578989139515454, val em: 0.06\n",
      "epoch: 51, train loss: 0.9802473179062384, val loss: 3.1769548255507836, val f1: 0.1309695830485304, val em: 0.06\n",
      "epoch: 52, train loss: 0.961854593118325, val loss: 3.3774336713494266, val f1: 0.12067878028404344, val em: 0.05\n",
      "epoch: 53, train loss: 0.9441258836153156, val loss: 3.449570563122397, val f1: 0.13251211361737678, val em: 0.06\n",
      "epoch: 54, train loss: 0.9270290306414126, val loss: 3.2996916591643823, val f1: 0.10618170426065163, val em: 0.04\n",
      "epoch: 55, train loss: 0.9105322636936855, val loss: 3.4253406345355324, val f1: 0.1390409356725146, val em: 0.06\n",
      "epoch: 56, train loss: 0.8946065997514581, val loss: 3.535248061945895, val f1: 0.1446203007518797, val em: 0.07\n",
      "epoch: 57, train loss: 0.8792232454733444, val loss: 3.6285539077976137, val f1: 0.11539807852965747, val em: 0.04\n",
      "epoch: 58, train loss: 0.8643558133446847, val loss: 3.4407341283137796, val f1: 0.1408454469507101, val em: 0.07\n",
      "epoch: 59, train loss: 0.8499793272482623, val loss: 3.546551345780026, val f1: 0.13001211361737677, val em: 0.07\n",
      "epoch: 60, train loss: 0.8360703587897655, val loss: 3.714508827753598, val f1: 0.10790100250626568, val em: 0.04\n",
      "epoch: 61, train loss: 0.8226068004534209, val loss: 3.626579650198837, val f1: 0.12138585099111415, val em: 0.05\n",
      "epoch: 62, train loss: 0.8095679335293274, val loss: 3.7955905494307807, val f1: 0.13643859649122805, val em: 0.06\n",
      "epoch: 63, train loss: 0.7969341677633132, val loss: 3.6770438384267616, val f1: 0.14506474519632415, val em: 0.07\n",
      "epoch: 64, train loss: 0.7846871541806376, val loss: 3.707863560092592, val f1: 0.12534544695071012, val em: 0.05\n",
      "epoch: 65, train loss: 0.7728095748250084, val loss: 3.848934483832709, val f1: 0.13595363408521302, val em: 0.06\n",
      "epoch: 66, train loss: 0.7612851198922099, val loss: 3.897325915180263, val f1: 0.12456766917293233, val em: 0.06\n",
      "epoch: 67, train loss: 0.7500983569370131, val loss: 3.9980874859698816, val f1: 0.16387426900584795, val em: 0.07\n",
      "epoch: 68, train loss: 0.7392347708858732, val loss: 3.909330714517273, val f1: 0.10856766917293235, val em: 0.04\n",
      "epoch: 69, train loss: 0.7286806685107182, val loss: 3.8780646223812436, val f1: 0.11995363408521303, val em: 0.05\n",
      "epoch: 70, train loss: 0.7184230615621093, val loss: 3.8543876100399577, val f1: 0.1281261487050961, val em: 0.06\n",
      "epoch: 71, train loss: 0.7084497060208907, val loss: 4.098705457243632, val f1: 0.1359594820384294, val em: 0.07\n"
     ]
    }
   ],
   "source": [
    "gradient_steps = 4\n",
    "train_losses = []\n",
    "val_epoch = {\n",
    "    \"loss\": [],\n",
    "    \"f1\": [],\n",
    "    \"em\": [],\n",
    "}\n",
    "wait = 10 * 60\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        images, texts, y = prepare_batch(batch)\n",
    "        logits = model(images)\n",
    "        loss = get_ctc_loss(logits, y)\n",
    "        loss.backward()\n",
    "        if idx % gradient_steps == 0 or idx == len(train_loader) - 1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        val_f1 = []\n",
    "        val_em = []\n",
    "        for batch in val_loader:\n",
    "            images, texts, y = prepare_batch(batch)\n",
    "            logits = model(images)\n",
    "            pt = get_predictions(logits)\n",
    "            loss = get_ctc_loss(logits, y)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            for t, p in zip(texts, pt):\n",
    "                val_f1.append(compute_f1(t, p))\n",
    "                val_em.append(compute_exact(t, p))\n",
    "    val_epoch[\"loss\"].append(sum(val_losses) / len(val_losses))\n",
    "    val_epoch[\"f1\"].append(sum(val_f1) / len(val_f1))\n",
    "    val_epoch[\"em\"].append(sum(val_em) / len(val_em))\n",
    "    print(f'epoch: {epoch}, train loss: {sum(train_losses) / len(train_losses)}, val loss: {sum(val_losses) / len(val_losses)}, val f1: {sum(val_f1) / len(val_f1)}, val em: {sum(val_em) / len(val_em)}')\n",
    "\n",
    "    if time.time() - start > wait:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'cnn_size={SIZE}.pth')\n",
    "history = val_epoch\n",
    "history.update({\n",
    "    \"train_losses\": train_losses\n",
    "})\n",
    "\n",
    "with open(f'cnn_size={SIZE}.json', 'w') as f:\n",
    "    json.dump(val_epoch, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
