{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from ignite.engine import (\n",
    "    Engine,\n",
    "    Events,\n",
    ")\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.contrib.handlers.neptune_logger import NeptuneLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodule import SROIETask2DataModule\n",
    "from model import TransformersEncoder\n",
    "from cnn import CNN as VisualModel\n",
    "from ctc import GreedyDecoder\n",
    "from igmetrics import ExactMatch, WordF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../trainer/tokenizer\")\n",
    "decoder = GreedyDecoder(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/israelcampiotti/Documents/Github/msc/tmp-master/SROIETask2\"\n",
    "dm = SROIETask2DataModule(\n",
    "    root_dir=os.path.join(DATA_PATH, \"data\"),\n",
    "    label_file=os.path.join(DATA_PATH, \"data.json\"),\n",
    "    tokenizer=tokenizer,\n",
    "    height=32,\n",
    "    num_workers=4,\n",
    "    train_bs=2,\n",
    "    valid_bs=2,\n",
    "    val_pct=0.001,\n",
    "    max_width=None,\n",
    "    do_pool=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(\"fit\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRModel(torch.nn.Module):\n",
    "    def __init__(self, visual_model, rec_model):\n",
    "        super().__init__()\n",
    "        self.visual_model = visual_model\n",
    "        self.rec_model = rec_model\n",
    "\n",
    "    def forward(self, images, attention_mask=None):\n",
    "        features = self.visual_model(images)\n",
    "        logits = self.rec_model(features, attention_mask=attention_mask)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_model = VisualModel()\n",
    "rec_model = TransformersEncoder(vocab_size=tokenizer.vocab_size)\n",
    "model = OCRModel(vis_model, rec_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader =  dm.val_dataloader()\n",
    "train_loader = val_loader # dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    images, labels, attention_mask, attention_image = batch\n",
    "\n",
    "    logits = model(images, attention_image)\n",
    "\n",
    "    input_length = attention_image.sum(-1)\n",
    "    target_length = attention_mask.sum(-1)\n",
    "\n",
    "    logits = logits.permute(1, 0, 2)\n",
    "    logits = logits.log_softmax(2)\n",
    "\n",
    "    loss = criterion(logits, labels, input_length, target_length)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(engine, batch):\n",
    "    model.eval()\n",
    "    images, labels, attention_mask, attention_image = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(images, attention_image)\n",
    "\n",
    "    decoded_ids = logits.argmax(-1).squeeze(0)\n",
    "    if len(decoded_ids.shape) == 1:\n",
    "        decoded_ids = decoded_ids.unsqueeze(0)\n",
    "    decoded = [\n",
    "        decoder(dec, att) for dec, att in zip(decoded_ids, attention_image)\n",
    "    ]\n",
    "    y_pred = tokenizer.batch_decode(decoded, skip_special_tokens=True)\n",
    "    y = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return y_pred, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluator = Engine(val_step)\n",
    "validation_evaluator = Engine(val_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExactMatch().attach(train_evaluator, \"accuracy\")\n",
    "ExactMatch().attach(validation_evaluator, \"accuracy\")\n",
    "WordF1().attach(train_evaluator, \"f1\")\n",
    "WordF1().attach(validation_evaluator, \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @trainer.on(Events.EPOCH_COMPLETED)\n",
    "# def log_training_results(engine):\n",
    "#     train_evaluator.run(train_loader)\n",
    "#     metrics = train_evaluator.state.metrics\n",
    "#     avg_accuracy = metrics['accuracy']\n",
    "#     print(f\"Training Results - Epoch: {engine.state.epoch}  Avg accuracy: {avg_accuracy:.3f}\")\n",
    "    \n",
    "def log_validation_results(engine):\n",
    "    validation_evaluator.run(val_loader)\n",
    "    metrics = validation_evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    print(f\"Validation Results - Epoch: {engine.state.epoch}  Avg accuracy: {avg_accuracy:.3f}\")\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(dirname='models', filename_prefix='deberta-ocr', n_saved=2, create_dir=True, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'model': model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_logger = NeptuneLogger(\n",
    "    project=\"i155825/OCRMsc\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhOGUyY2VlOS1hZTU5LTQ2NGQtYTY5Zi04OGJmZWM2M2NlMDAifQ==\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED,\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"loss\": loss},\n",
    ")\n",
    "\n",
    "neptune_logger.attach_output_handler(\n",
    "    validation_evaluator,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"validation\",\n",
    "    metric_names=[\"f1\", \"accuracy\"],\n",
    "    global_step_transform=global_step_from_engine(trainer),  \n",
    ")\n",
    "\n",
    "neptune_logger[\"code\"].upload_files([\"../trainer/*.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "pbar.attach(trainer, output_transform=lambda x: {'loss': x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
